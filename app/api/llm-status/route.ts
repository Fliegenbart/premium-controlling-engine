/**
 * LLM Status API - Check availability of Claude and Ollama
 */

import { NextResponse } from 'next/server';
import { getHybridLLMService } from '@/lib/llm/hybrid-service';

export async function GET() {
  try {
    const llm = getHybridLLMService();
    const status = await llm.getStatus();

    return NextResponse.json({
      success: true,
      status: {
        ollamaAvailable: status.ollamaAvailable,
        claudeAvailable: status.claudeAvailable,
        preferredProvider: status.preferredProvider,
        activeProvider: status.activeProvider,
        message: status.ollamaAvailable
          ? 'ðŸŸ¢ Lokales LLM (Ollama) verfÃ¼gbar - Daten bleiben On-Premise'
          : status.claudeAvailable
          ? 'ðŸŸ¡ Cloud LLM (Claude) aktiv - Daten werden verschlÃ¼sselt Ã¼bertragen'
          : 'ðŸ”´ Kein LLM verfÃ¼gbar - Nur regelbasierte Kommentare',
      },
    });
  } catch (error) {
    return NextResponse.json({
      success: false,
      error: (error as Error).message,
      status: {
        ollamaAvailable: false,
        claudeAvailable: false,
        preferredProvider: 'none',
        activeProvider: 'none',
        message: 'ðŸ”´ Fehler beim PrÃ¼fen der LLM-VerfÃ¼gbarkeit',
      },
    });
  }
}
